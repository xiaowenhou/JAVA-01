**第二十三课作业必做题**

题目：配置 redis 的主从复制，sentinel 高可用，Cluster 集群

环境：用三台虚拟机， 分别安装三个linux系统环境以及三个Redis

**一、配置reids的主从复制**

采用一主两从的架构

1、三台机器都配置

```
#开启守护模式
daemonize yes

#关闭保护模式
protected-mode no

#开启任何主机都可以访问本机器（实际生产中可能只会配置内网中相互通信的机器IP， 并且会开启用户名和密码验证）
bind 0.0.0.0
```

2、从机中配置

```
#告诉redis，自己的主机是192.168.198.110这台机器上的redis机器
replicaof 192.168.198.110 6379
```

​	如果要配置读写分离，在**三台机器**上都配置

```
#表示从机只读
replica-read-only yes
```

​	配置读写分离之后的效果：在从机上进行写操作时会报错

![image-20210410180704538](C:\Users\xiaowenhou\AppData\Roaming\Typora\typora-user-images\image-20210410180704538.png)

**二、sentinel高可用**

在sentinel.conf文件中配置

```
#关闭保护模式
protected-mode no
#sentinel端口
port 26379
#sentine为守护进程
daemonize yes
#pid文件的位置
pidfile "/var/run/redis-sentinel.pid"
#日志
logfile "/xiaowenhou/server/redis/logs/sentinel.log"
#表示哨兵监控test-master这个集群（哨兵可以监控多个集群）， 该集群的master节点是192.168.198.110， 端口是6379， 最后的2表示当两个哨兵都认为主宕机之后就判定主机客观下线， 进行故障转移
sentinel monitor test-master 192.168.198.110 6379 2
# 执行故障转移的timeout超时时长，默认是三分钟， 配置时间越长， 则故障转移时集群不可用时间越长， 配置时间越短， 则哨兵越敏感，容易发生误判
sentinel down-after-milliseconds test-master 5000
# 当进行failover时，配置所有slaves指向新的master所需的最大时间
sentinel failover-timeout test-master 60000


#以下为执行一次故障转移之后sentinel自动写进去的日志配置
sentinel config-epoch test-master 14
sentinel leader-epoch test-master 14
sentinel known-replica test-master 192.168.198.100 6379
sentinel known-replica test-master 192.168.198.120 6379
sentinel known-sentinel test-master 192.168.198.120 26379 337efcde16e3dd86dcd28312c11aab1fec4d6b9d
sentinel known-sentinel test-master 192.168.198.110 26379 92dcc82b47a81cef62350ecb57955c09b51ebf55
sentinel current-epoch 14
                           
```

三、Redis集群

​	采用官网的Redis Cluster方式， 集群使用三主三从， 三个虚拟机中各个主机的6380端口为主节点， 6381为从节点。

​	配置redis，主要修改端口， 设置为守护进程模式，设置redis的日志

​	添加cluster的配置后启动

```
cluster-enable yes
```

![image-20210410203241091](C:\Users\xiaowenhou\AppData\Roaming\Typora\typora-user-images\image-20210410203241091.png)

​	创建redis集群：

​	注意创建之前清空各个redis库， 并且删除所有的dump.rdb文件和aof文件

```
[root@node01 bin]# ./redis-cli --cluster create 192.168.198.100:6380 192.168.198.110:6380 192.168.198.120:6380 192.168.198.100:6381 192.168.198.110:6381 192.168.198.120:6381 --cluster-replicas 1
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 192.168.198.110:6381 to 192.168.198.100:6380
Adding replica 192.168.198.120:6381 to 192.168.198.110:6380
Adding replica 192.168.198.100:6381 to 192.168.198.120:6380
M: ad67ff9e44b5a544df7d39d6d32084951e8e65a7 192.168.198.100:6380
   slots:[0-5460] (5461 slots) master
M: 001a16987217a601d43dea09e2f988394c3868ec 192.168.198.110:6380
   slots:[5461-10922] (5462 slots) master
M: 0e79cb14ca16fd0f38089ad6f1a183cb9f430608 192.168.198.120:6380
   slots:[10923-16383] (5461 slots) master
S: fb9b8eed8c6208e86fdef9c793873e0e1c0185d5 192.168.198.100:6381
   replicates 0e79cb14ca16fd0f38089ad6f1a183cb9f430608
S: 1f23f84f649db7e5bc7baf54d089bc6b15aac970 192.168.198.110:6381
   replicates ad67ff9e44b5a544df7d39d6d32084951e8e65a7
S: 032bc54296efd007033b173d8549bd3dc1568acd 192.168.198.120:6381
   replicates 001a16987217a601d43dea09e2f988394c3868ec
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
....
>>> Performing Cluster Check (using node 192.168.198.100:6380)
M: ad67ff9e44b5a544df7d39d6d32084951e8e65a7 192.168.198.100:6380
   slots:[0-5460] (5461 slots) master
   1 additional replica(s)
S: 032bc54296efd007033b173d8549bd3dc1568acd 192.168.198.120:6381
   slots: (0 slots) slave
   replicates 001a16987217a601d43dea09e2f988394c3868ec
M: 0e79cb14ca16fd0f38089ad6f1a183cb9f430608 192.168.198.120:6380
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
S: 1f23f84f649db7e5bc7baf54d089bc6b15aac970 192.168.198.110:6381
   slots: (0 slots) slave
   replicates ad67ff9e44b5a544df7d39d6d32084951e8e65a7
M: 001a16987217a601d43dea09e2f988394c3868ec 192.168.198.110:6380
   slots:[5461-10922] (5462 slots) master
   1 additional replica(s)
S: fb9b8eed8c6208e86fdef9c793873e0e1c0185d5 192.168.198.100:6381
   slots: (0 slots) slave
   replicates 0e79cb14ca16fd0f38089ad6f1a183cb9f430608
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
[root@node01 bin]# 

```

连接集群：

```
[root@node01 bin]# ./redis-cli -p 6380 -c
```

查看集群信息

```
127.0.0.1:6380> cluster info
cluster_state:ok
cluster_slots_assigned:16384
#16384个槽
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
#6个已知节点
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:186
cluster_stats_messages_pong_sent:187
cluster_stats_messages_sent:373
cluster_stats_messages_ping_received:182
cluster_stats_messages_pong_received:186
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:373
127.0.0.1:6380> 

```

查看集群节点

![image-20210410210354850](C:\Users\xiaowenhou\AppData\Roaming\Typora\typora-user-images\image-20210410210354850.png)

可见有三主三从

官方提供的RedisCluster集群带有故障转移， 副本漂移等功能， 只是需要添加节点的时候的批量操作比较麻烦

